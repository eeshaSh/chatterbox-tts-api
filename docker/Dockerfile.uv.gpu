# Use NVIDIA CUDA runtime as base for better GPU support
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV DEBIAN_FRONTEND=noninteractive

# Install Python 3.11 and system dependencies
RUN apt-get update && apt-get install -y \
    software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-distutils \
    git \
    wget \
    curl \
    build-essential \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

# Set working directory
WORKDIR /app

# Create virtual environment
RUN uv venv --python 3.11

# Activate the virtual environment for subsequent RUN commands
ENV VIRTUAL_ENV=/app/.venv
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Install PyTorch with CUDA support using uv
# RUN uv pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
# TODO put this in a requirements file
# Install the exact CUDA‑12.4 wheels
# RUN uv pip install \
#     torch==2.6.0+cu124 \
#     torchvision==0.21.0+cu124 \
#     torchaudio==2.6.0+cu124 \
#     --index-url https://download.pytorch.org/whl/cu124
# Inside your Dockerfile (after you have created/activated a virtual‑env)
RUN apt-get update && apt-get install -y --no-install-recommends \
    git build-essential cmake ninja-build && \
    rm -rf /var/lib/apt/lists/*
RUN pip install --upgrade pip setuptools wheel && \
    pip install torch==2.7.1+cu128 \
                torchvision==0.18.1+cu128 \
                torchaudio==2.7.1+cu128 \
    -f https://download.pytorch.org/whl/torch_stable.html
RUN git clone https://github.com/Dao-AILab/flash-attention.git /opt/flash-attn && \
    cd /opt/flash-attn && \
    git checkout v2.5.8 &&
RUN cd /opt/flash-attn && \
    python setup.py install && \
    # clean up to keep the image small
    cd / && rm -rf /opt/flash-attn
# Transformers ≥ 4.41.0 and Accelerate ≥ 0.30.0 contain the clone‑cache fix
RUN uv pip install "transformers>=4.41.0" "accelerate>=0.30.0"
RUN uv pip install "bitsandbytes>=0.43.0"

ENV TORCHINDUCTOR_MAX_BLOCK_SIZE=64   # keeps the autotuner from picking a block that exceeds shared memory

# Install base dependencies first
RUN uv pip install setuptools fastapi uvicorn[standard] python-dotenv python-multipart requests psutil pydub sse-starlette

# Install chatterbox-tts — with the breaking fix (pkuseg package exclusion) 
# RUN uv pip install git+https://github.com/randombk/chatterbox-vllm.git@master
# RUN uv pip install git+https://github.com/eeshaSh/chatterbox-faster-mods.git@faster
RUN uv pip install git+https://github.com/rsxdalv/chatterbox.git@faster
# RUN uv pip install git+https://github.com/travisvn/chatterbox-multilingual.git@exp

# Copy application code
COPY app/ ./app/
COPY main.py ./

# Copy voice sample if it exists (optional, can be mounted)
COPY voice-sample.mp3 ./voice-sample.mp3

# Create directories for model cache, voice library, and long text data (separate from source code)
RUN mkdir -p /cache /voices /data/long_text_jobs

# Set default environment variables (prefer CUDA)
ENV PORT=4123
ENV EXAGGERATION=0.5
ENV CFG_WEIGHT=0.5
ENV TEMPERATURE=0.8
ENV VOICE_SAMPLE_PATH=/app/voice-sample.mp3
ENV MAX_CHUNK_LENGTH=280
ENV MAX_TOTAL_LENGTH=3000
ENV DEVICE=cuda
ENV MODEL_CACHE_DIR=/cache
ENV VOICE_LIBRARY_DIR=/voices
ENV HOST=0.0.0.0

# Long text TTS settings
ENV LONG_TEXT_DATA_DIR=/data/long_text_jobs
ENV LONG_TEXT_MAX_LENGTH=100000
ENV LONG_TEXT_CHUNK_SIZE=2500
ENV LONG_TEXT_SILENCE_PADDING_MS=200
ENV LONG_TEXT_JOB_RETENTION_DAYS=7
ENV LONG_TEXT_MAX_CONCURRENT_JOBS=3

# NVIDIA/CUDA environment variables
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose port
EXPOSE ${PORT}

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5m --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Run the application using the virtual environment Python
CMD ["/app/.venv/bin/python", "main.py"] 